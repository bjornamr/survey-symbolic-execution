
\begin{figure}[t]
\begin{small}
\begin{center}
\begin{tabular}{c}
\begin{lstlisting}[basicstyle=\ttfamily\small]
// N symbolic branches 
if (input[0] < 42) [...]
[...]
if (input[N-1] < 42) [...]

// symbolic loop
strcpy(dest, input); 

// M symbolic branches
if (input[N] < 42) [...]
[...]
if (input[N+M-1] < 42) [...]
\end{lstlisting}
\end{tabular}
\end{center}
\end{small}
\caption{\label{fig:preconditioned} Preconditioned symbolic execution example~\protect\cite{AEG-NDSS11}.}
\end{figure}

\begin{figure}[t]
\begin{center}
\begin{tabular}{c}
\begin{lstlisting}[basicstyle=\ttfamily\small]
1.  void foo(int x, int y) {
2.     if (x < 5)
3.        y = y * 2;
4.     else
5.        y = y * 3;
6.     return y;
7.  }
\end{lstlisting}
\end{tabular}
\end{center}
\caption{State merging example.}
\label{fi:example-state-merging}
\end{figure}



\begin{itemize}

\item Exponential in branching structure: in the example, 3 variables, 8 paths (can we generalize? $t$ variables, $2^t$ paths?)

\begin{verbatim}
1. int a = ?, b = ?, c = ?; // symbolic
2. if (a) ... else ...;
3. if (b) ... else ...;
4. if (c) ... else ...;
\end{verbatim}

\item Loops on symbolic variables even worse

\begin{verbatim}
1. int a = ?; // symbolic
2. while (a) do ...;
3.
\end{verbatim}
\item Potentially $2^{31}$ paths through loop!

\end{itemize}


%%% INTRO

\iffalse
  \item {\em Subroutines and recursion}: how does the symbolic engine handle subroutines and recursive calls?
In order to support subroutines, the execution state is typically provided with an execution stack. Consider the following recursive example:
    \begin{lstlisting}[basicstyle=\ttfamily\small]
    1.  int fac(int n) {
    2.    if (n <= 1) return 1;
    3.    return n*fac(n - 1);
    3.  }
    \end{lstlisting}
Note that each distinct value of {\tt n} leads to a distinct control flow path. Since {\tt n} can assume up to  $2^{31} - 1$ positive values, a symbolic engine would create as many execution states to cover all the possible paths.
%This code can easily lead to a very large number of states: a new state would be indeed created each time the branch at line 2 is not taken. Since variable {\tt n} can assume up to  $2^{31} - 1$ positive values, the symbolic engine would create as many execution states to cover all the possible paths.
 %Indeed, the number of executions states is related to the number of times that the conditional branch on line 2 is not taken. 
 \vspace{1mm}
 \fi

%%% MEMORY MODELS
\vspace{2em}\mynote{[D] Previous text starts here}In other words, a symbolic engine may see symbols as distinct objects (i.e., each symbol is a distinct array of a specific size) or as pointers to a flat memory (i.e., index-based memory). Although the latter approach may seem more natural since it is akin to a concrete execution model, the former has been proved to be very effective in many scenarios since the symbolic constraints generated when using this approach are {\em easier} to parse for some solvers.

% ---------------------------------------------------------------------------------------------------
\subsection{Index-based memory}

\begin{itemize}
  \item memory is a map $\pi : I \to E$ from 32-bit indexes ($i \in I$) to expressions ($e \in E$)
  \item load expressions $e = load(\pi, i)$: $i$ indixes $\pi$ and the loaded value $e$ represents the contents of the $i$-th memory cell
  \item store expressions $store(\pi, i, e)$: a new memory $\pi'$ where $i$ is mapped to $e$, i.e., $\pi' = \pi[i \gets e]$
\end{itemize}
When using this memory model, handling of arbitrary symbolic indexes is notoriously hard, since a symbolic index may reference any cell in memory. Two approaches can be pursued: (a) concretization of the index where only a single value is evaluated for the index (see Section~\ref{concolic-execution} for more details), (b) fully symbolic memory where any possible value for the index is evaluated (e.g., \cite{BAP-CAV11}). The former is often excessively limiting (many paths are pruned away), while the latter is hard to make scalable. To overcome the limitations of both approaches, \cite{MAYHEM-SP12} models memory {\em partially}: symbolic writes are always concretized, while symbolic reads are allowed to be modeled symbolically.

\paragraph{Memory objects} Whenever there is a symbolic read, an immutable memory object $\mathcal{M}$ is generated:  it contains all values that could be accessed by the index, i.e., $\mathcal{M}$ is a partial snapshot of the global memory $\pi$. In practice, \cite{MAYHEM-SP12} reasons on $\mathcal{M}[i]$ instead of reasoning on $\pi[i]$, since the former is typically smaller than the latter.

\paragraph{Memory object bounds resolution}\mynote{Move paragraphs to Section~\ref{constraint-optimizations}?} \cite{MAYHEM-SP12} trades accuracy with scalability by resolving the bounds $[\mathcal{L}, \mathcal{U}]$ of the memory region, where $\mathcal{L}$ is lower and $\mathcal{U}$ is the upper bound for the index $i$. Initially, $\mathcal{L} \in [0, 2^{32}-1]$. The solver is used to test if $i < \frac{2^{32}-1}{2}$ makes the path constrains unsatisfiable. If satisfiable then $\mathcal{L} \in [0, \frac{2^{32}-1}{2}]$, otherwise $\mathcal{L} \in [\frac{2^{32}-1}{2}, 2^{32}-1]$. The same strategy is repeated as much as possible. This is akin to a binary search algorithm. Whenever the bounds become reasonable, a memory object is generated. Unfortunately, there several cases where the bounds cannot be narrowed down drastically and thus other techniques can be used:
\begin{itemize}
  \item {\em value set analysis (VSA)}
  \item {\em refinement cache}
  \item {\em lemma cache}
  \item {\em index search trees (IST)}
  \item {\em bucketization with linear functions}
\end{itemize}
Whenever the size of the memory object exceeds a threshold (e.g., $|\mathcal{M}| \geq 1024$), then~\cite{MAYHEM-SP12} concretizes the index. However, instead of choosing a random value for the index, it tries to assign some {\em interesting} value (e.g., test it if it makes sense for it to be equal to an invalid memory address which can be exploited for security purposes).

% ---------------------------------------------------------------------------------------------------
\subsection{Object-based memory}

\cite{STP-TR07} is a decision procedure for bitvectors and arrays. Memory is seen as untyped bytes. Three data types are available:
\begin{itemize}
  \item {\em booleans}
  \item {\em bitvectors}: a fixed-length sequence of bits
  \item {\em arrays of bitvectors}
\end{itemize}
Most linear and non-linear operations are mapped to bitvector constraints. Conditional branches are transformaed into {\em multiplexers}, which are similar to C ternary operator. Bitvector operations are translated into operations to individual bits. Floating-points data types are not supported. Expresions types:
\begin{itemize}
  \item {\em formulas}, which have boolean values. They are converted into DAGs of single bit operations, where expressions with identical structure are represented uniquely (an hash table is maintain to track of existing expressions and lookups are performed on it when a new expression is created).
  \item {\em terms}, which have bitvectors values. They are converted into vectors of boolean formulas consisting entirely of single bit operations.
\end{itemize}

\paragraph{Mapping C code to STP primitives} Each C data block is represented symbolically as an array of 8-bit bitvectors. Typed operations in C generated constraints on the symbolic data (i.e., typeness is not actually known to~\cite{STP-TR07}). A mapping table is maintained to track symbolic data:
\begin{itemize}
  \item each input array $b$ is associated to a symbolic identically-size array $b_{sym}$ (i.e., the address of the C variable $b$ is mapped to the symbolic array $b_{sym}$ that is composed by $|b|$ 8-bit elements)
  \item $v = e$: an assignment expression, where $e$ is an expression that involves one or more symbolic data, adds a mapping between the address of $v$ and the generated symbolic expression $e_{sym}$. If $v$ is overwritten with a constant value or deallocated then this mapping is removed.
  \item $b[e]$: $b_{sym}$ is allocated, initializing it with the (constant) contents of $b$ (only if $b$ is actually initialized by a previous C statement). A mapping is added until $b$ is not deallocated.
\end{itemize}

\paragraph{Expression evaluation} An expression $e$ that contains some symbolic data can be seen as:
\[ l_1~~op_1~~l_2~~op_2~~l_3~~... \]
Each $l_i$ is evaluated in the following way:
\begin{itemize}
  \item if $l_i$ is concrete: the concrete value is used in the expression (e.g., if 4-byte $b$ is equal to 4 then the constant $000000...0100$ is used)
  \item otherwise: a concatenation of all the symbolic bytes of $l_i$ is used (e.g., $b_{sym}[0] @b_{sym}[1] @ $ $b_{sym}[2] @ b_{sym}[3] @ ...$)
\end{itemize}
Pointers are seen as array reference at some offset. This means that allocation sites as well as pointer arithmetic expressions must be instrumented in order to track where a pointer can point to. Notice that double dereferences ({\tt **p}) force~\cite{STP-TR07} to concretize the first dereference ({\tt *p}). 

\paragraph{Fast array transformations}\mynote{Move paragraphs to Section~\ref{constraint-optimizations}?} Some transformations are needed to make\cite{STP-TR07} reason only on a purely functional language. In particular:
\begin{itemize}
  \item {\em read-over-write}: eliminates all write operations
    \[ read(write(A, i, v), j) \implies ite(i = j, v, read(A, j)) \]
    where $ite(a,b,c)$ is a ternary operator (i.e., if $a$ then $b$ else $c$). Notice that a write of a location without a subsequent read of the same location can be ignored.
  \item {\em read elimination}:
    \[ (read(A, i) = e_1) \wedge (read(A, j) = e_2) \]
    will be transformed in:
    \[ (v_1 = e_1) \wedge (v_2 = e_2) \wedge (i=j \implies v_1 = v_2) \]
  \item {\em array substitution optimization}
  \item {\em array-based refinement}
  \item {\em simplifications}: boolean or mathematical identities
\end{itemize} 

%%% COMPLEX OBJECTS

It is common to use {\em lazy initialization} for handling complex memory objects. Notice that if lazy initialization is not used by the symbolic engine, then it is likely that it has to treat complex objects completely symbolically, requiring a constraint solver that is able to solve the resulting constraints. In other words, the solver must support some form of theory of data structures or arrays.

\paragraph{Recursive data structures} Lazy initialization over recursive data structure works~\cite{PV-JSTTT09} as follows:
\begin{itemize}
  \item whenever an instance method is called over a symbolic object, the object is created with uninitialized fields
  \item whenever an uninitialized field of a non-native type is accessed, the symbolic engine non deterministically initialize it with: (a) a {\tt null} reference, (b) a reference to a previously created object, (c) a reference to a new object with uninitialized fields. 
  \item whenever an uninitialized field of a native type\mynote{this is my guess, it's not clear explained in~\cite{PV-JSTTT09} - [D] correct, see KPV-TACAS03}) is accessed, a new symbolic value is introduced. 
\end{itemize}

\mynote{Merge discussion of {\em lazy initialization} done in Section~\ref{under-constrinained}.}

Notice that if some input preconditions~\ref{precontioned-symbolic-execution} has been set, lazy evaluation should consider and exploit them. For instance, the object may have been marked as acyclic and thus fewer possible alternatives should be considered when initializing a uninitialized field. Further complication~\cite{PV-JSTTT09} may arise for programs that perform destructive updates. 

\paragraph{Native code} Several programming languages provide data types that can be very complex. For instance, Java provides the {\tt String} type. Prior works (see, e.g.,~\cite{SHZ-TAIC07}) has presented some approaches for addressing these issues.

\paragraph{Input arrays}
\mynote{Which paper???}It is common that programs may have loops bounded by the length of some input arrays. If this length cannot be statically determined then the symbolic engine may non-deterministically choose when to stop a loop. This means that the number of entries in a symbolic array does not need to be chosen as soon as the symbolic array is created, but this choice can be postponed.

%%% ENVIRONMENT

%In order to analyze a program, a symbolic executor has typically to take into account the whole environment surrounding it, including system libraries, devices etc.
%Environment can be seen as an input source. Since it can be unfeasible to analyze all possible interactions with the environment, it is common to model these interactions, emulating their behaviors and their side-effects. The main intuition is that models understand the semantics of the desired actions well enough to generate the required constraints.\\

\iffalse
%\cite{KLEE-OSDI08}: \begin{itemize}
%\item {\em file system}: operations on concrete files are actually performed. Operations on symbolic files are emulated modeling a simple symbolic file system, which is private for each execution state. Symbolic file system is a directory with $N$ symbolic files. Users specify both the number of files and their sizes. Any operation on a unconstrained symbolic file will generate $N+1$ branches: one for each symbolic file, plus one for a failing scenario. Emulation done at library level, not system call level. This make symbolic execution simpler (no need to symbolically execute library code) but assumption that library code is correct. If needed, library code is tested separately.
%\item {\em environment failures}: \cite{KLEE-OSDI08} emulates failures (e.g., failures of {\tt write}). This is optional since some applications may be not sensitive to environment failure.
 %\item {\em re-running test cases}: inputs which may crash an application may depend on the environment failures. To force concrete execution towards failures, \cite{KLEE-OSDI08} exploits {\tt ptrace}.
%\end{itemize}

\cite{AEG-NDSS11}:
\begin{itemize}
  \item {\em symbolic files}: emulation of {\tt open}, {\tt read}, {\tt write}, and similar other system calls. Similar to~\cite{KLEE-OSDI08}.
  \item {\em symbolic sockets}: emulation of {\tt socket}, {\tt bind}, {\tt accept}, {\tt send}, and similar other system calls. 
  \item {\em environment variables}: a complete summary of all possible results (concrete values, fully symbolic, and failures) of {\tt get\_env}.
  \item {\em library function calls and system calls}: emulation of more than 70 library routines and system calls. In particular, formatting functions (e.g., {\tt fprintf}) are emulated to capture buffer overflows.
\end{itemize}
\fi


%%%% UNDER-CONSTRAINED

\iffalse % [D] original text here, while commented text has been moved to sandbox.tex
%By isolating a function from the rest of the program, we can perform symbolic execution on it.
A possible approach to avoid path explosion with function calls is to symbolically execute a function in isolation. The results of the analysis can then be exploited when any other code region is symbolically executed and a call to the function is present. However, errors detected in the isolated function may be false positives since the input may never assume certain values (e.g., a null pointer) when the function is executed in the context of the full program. Some prior works, e.g., \cite{CS-ICSE05}\mynote{check this paper}, first analyze the code in isolation and then test the generated crashing inputs using concrete executions. % However, errors detected in the isolated function may be false positives since some of the input values may never been valid when the function is actually executed in the context of the full program

{\em Under-constrained symbolic execution}~\cite{ED-ISSTA07} is a technique that performs symbolic execution of an isolated function and explicitly marks which symbols are {\em under-constrained} (i.e., their symbolic values might violate preconditions due to missing constraints) to distinguish them from {\em exactly constrained} symbols.

Errors due to concrete values and exactly constrained symbols are treated as true positives. On the other hand, errors due to under-constraining are treated as true positives only if {\em all} the solutions to the currently known constraints on the symbols cause the error to occur. Otherwise the negation of the error is added to the constraint set and the symbolic execution of the isolated function is continued. In other words, an error is \mynote{[D] added promptly} promptly reported if and only if it is {\em context-insensitive}. Notice that a symbol may initially be under-constrained and then become exactly constrained. For instance, consider the following piece of code:

    \begin{lstlisting}[basicstyle=\ttfamily\small]
    assert(a != 0); // no knowledge about a
    a = 0;          // from now on a's value is known
    assert(a != 0); // error always hits: context-insensitive! 
    \end{lstlisting}

If {\tt a} is marked as under-constrained, then the first {\tt assert} will not trigger an error: indeed, there is at least one possible value for the symbol associated to {\tt a} that does not hit the error. Conversely, the second {\tt assert} will always trigger an error since {\tt a} has a concrete value and is not under-constrained anymore.

Although this technique is not able to find {\em all} the possible errors in a function, it can still find interesting bugs. Moreover, since symbolically executing a whole program may be unfeasible, this technique allows for testing a large number of lines of code in a reasonable amount of time. In particular, this technique allows an engine to skip code: if a function or any other construct (e.g., a loop) may be troublesome for symbolic execution, it can be skipped by just marking the locations affected by it as under-constrained. However, a possible implementation issue is given by the propagation of under-constrained symbols: given an instruction of the form {\tt if (s < t)}, if {\tt t} is under-constrained while {\tt s} is exactly constrained then when the symbolic execution proceeds through the two possible branches, {\tt s} must be marked as under-constrained. Some optimizations may thus be needed in order to minimize this propagation effect.
\fi

%%% STATE MERGING
\paragraph{Selective state merging} Intermediate merging solutions can adopt heuristics for driving both merging decisions and CFG exploration. Generating larger symbolic expressions and possibly extra solvers invocations can outweigh the benefit of having fewer states, leading to poorer overall performance~\cite{HSS-RV09,KKB-PLDI12}. Moreover, in order to maximize the opportunities for merging a symbolic execution engine should traverse the CFG in a topological order, which prevents search exploration strategies aiming at prioritizing more ``interesting'' states over others. % denies search exploration
%(e.g., \cite{KKB-PLDI12} and \cite{VERITESTING-ICSE14}) 
Recent works have thus introduced novel techniques to tackle these issues: 
\begin{itemize}

\item {\em query count estimation}~\cite{KKB-PLDI12} identifies state merges that can reduce exploration time. This technique relies on a simple form of static analysis to identify how often each variable is used in branch conditions past any given point in the CFG. The estimate is used as a proxy for the number of solver queries that a given variable is likely to be part of. When two states are sufficiently similar, the overhead from solving more complex queries is likely to be outweighed by the savings from exploring fewer paths;

  \item {\em dynamic state merging}~\cite{KKB-PLDI12} efficiently combines static state merging with common search heuristics. This technique allows merging of states that do not share the same program location. This is useful, for instance, for unbounded loops for which search-based symbolic execution engines would employ search strategies that prioritize exploring new code over unrolling, while static state merging would require a depth-first exploration and thus fully unroll the possibly infinitely many iterations of the loop. Dynamic state merging can consider for merging states that are likely to become similar in a small number of execution steps: this is likely to happen if one state is similar to one of the predecessors of the other. The intuition behind the algorithm is that if two states are similar, then also their respective successors are likely to be similar after a few steps;

  \item {\em veritesting}~\cite{VERITESTING-ICSE14} dynamically identifies statements that generate formulas that are easy for a solver. Using a dynamically recovered CFG, it detects pieces of code that do not contain system calls, indirect jumps, or other statements that are difficult for static analysis. In particular, frontiers of hard-to-analyze statements are identified. Easy-to-analyze statements are analyzed maintaining a single formula describing all the merged states, while hard-to-analyze statements are evaluated using separate states as in traditional symbolic execution.
%Easy-to-analyze set of statements are then analyzed maintaining a single formula that describe all the merged states, while hard to analyze set of statements are evaluated using separate states pursuing the traditional symbolic execution approach.

\end{itemize}

%%% BINARY
\iffalse
\vspace{4em}
\mynote{Notes from D\&E start here} For software such as common off-the-shelf programs, neither users nor attackers have access to their source code: 

Challenges (e.g., ~\cite{BITBLAZE-ICISS08}):
\begin{enumerate}
\item Complexity of the instruction sets
\item Lack of a higher-level semantics (functions/CFG, types, buffers)
\item Obfuscation/dynamic code generation
\end{enumerate}

Symbolic techniques may work on the source code or on the binary code. However, it is not uncommon that both the former and the latter work by reasoning on an intermediate representation of the original code. For instance, ~\cite{KLEE-OSDI08} interprets the LLVM bytecode generated by compiling the source code, while~\cite{ANGR-SP16} reasons on the VEX IR that has been obtained by lifting the binary code.
\fi


%%% CONSTRAINTS

%Notice that the use of concrete values can also avoid to perform alias analysis on pointers, which is typically very expensive. Whenever meaningful, \cite{DART-PLDI05} tries to test both valid (not {\tt NULL}) and invalid ({\tt NULL}) input pointers in order to maximize bug detection. However~\cite{DART-PLDI05} will never artificially negate a branch if that condition cannot be exercised using a concrete input. In other words, both branches of an {\tt if} statement are considered only if they are both meaningful (more precisely: \cite{DART-PLDI05} is able to generate a valid input).

%Notice that~\cite{DART-PLDI05} may generate an input using a solver by considering only a subset of branch constraints. For instance, consider constraints ($C_1, C_2, C_3$) given by three nested branches: if $C_1$ is non linear (hard to solve), it needs only to generate a random input for taking $C_1$ and then use the solver for exploring path given by $(C_2, C_3)$. A traditional symbolic execution engine may get stuck at $C_1$ and give up after some time on {\em all} the derived path. Notice that whenever a concrete input is used to overcome a hard constraint, the overall approach become incomplete.

%~\cite{CKC-TOCS12} 
%Whenever a memory access with a symbolic pointer occurs,~\cite{CKC-TOCS12} determines the page referenced by the pointer. This information is then passed to the solver to help it. To make this even more effective, the page size is reduced as much as possible (e.g., 128 bytes).
%of constraints that might be hard or impossible to reason on for a solver. For instance, 

%A significant amount of the execution time of a symbolic engine is spent invoking the constraint solver.

\iffalse
\subsection{[OLD] Dealing with unsolvable constraints} 
\iffalse
Assume to start a concrete execution with a concrete input and in parallel symbolically execute the same program. Whenever a set of constraints cannot be solved by the constraint solver, then use the concrete value to proceed into at least one branch. Example taken from~\cite{CS-CACM13}:
    \begin{lstlisting}[basicstyle=\ttfamily\small]
    int non_linear(int v) {
      return (v * v) % 50;
    }
    \end{lstlisting}
The non-linear operation inside this function can be hard for a solver. Using a concrete execution, the engine can overcome this problem, but then the precision and completeness may be affected.
\fi

Notice that the use of concrete values can also avoid to perform alias analysis on pointers, which is typically very expensive. Whenever meaningful, \cite{DART-PLDI05} tries to test both valid (not {\tt NULL}) and invalid ({\tt NULL}) input pointers in order to maximize bug detection. However~\cite{DART-PLDI05} will never artificially negate a branch if that condition cannot be exercised using a concrete input. In other words, both branches of an {\tt if} statement are considered only if they are both meaningful (more precisely: \cite{DART-PLDI05} is able to generate a valid input). Notice that~\cite{DART-PLDI05} may generate an input using a solver by considering only a subset of branch constraints. For instance, consider constraints ($C_1, C_2, C_3$) given by three nested branches: if $C_1$ is non linear (hard to solve), it needs only to generate a random input for taking $C_1$ and then use the solver for exploring path given by $(C_2, C_3)$. A traditional symbolic execution engine may get stuck at $C_1$ and give up after some time on {\em all} the derived path. Notice that whenever a concrete input is used to overcome a hard constraint, the overall approach become incomplete.
\fi

% ---------------------------------------------------------------------------------------------------
\iffalse
\subsection{Solvers}
A list of constraint solvers\mynote{Table?}:
\begin{itemize}
  \item \cite{STP-TR07}: used by~\cite{EXE-CCS06,KLEE-OSDI08,MineSweeper-BOTNET08}, used by SPF
  \item \cite{Z3-TACS08}: used by~\cite{FIRMALICE-NDSS15,MAYHEM-SP12,SAGE-QUEUE12}
  \item \cite{DISSOLVER-TR03}: initially used by \cite{SAGE-NDSS08}
  \item \cite{PPL-SCP08}: used by \cite{CFB-ACSAC06}
  \item (incremental solver) \href{http://www.cs.nyu.edu/acsys/cvc3/}{CVC3}: an automatic theorem prover for Satisfiability Modulo Theories, used by SPF
  \item (incremental solver) \href{http://yices.csl.sri.com/}{Yices}: The Yices SMT Solver~\cite{YICES-CAV06}
  \item \href{http://choco-solver.org/}{CHOCO}: A Free and Open-Source Java Library for Constraint Programming, used by SPF
  \item \href{http://www.cs.brandeis.edu/~tim/Applets/IAsolver.html}{IAsolver}: the Brandeis Interval Arithmetic Constraint Solver, used by SPF
  \item \href{https://people.csail.mit.edu/akiezun/hampi/}{Hampi}: A Solver for String Constraints, used by SPF
  \item \href{https://www.cs.umd.edu/projects/omega/}{Omega}~\cite{OMEGA-SC91}, used by SPF
  \item \href{http://pagesperso.lina.univ-nantes.fr/~granvilliers-l/realpaver/}{RealPaver}: onlinear constraint solving \& rigorous global optimization, used by SPF
  \item \href{http://smtlib.cs.uiowa.edu/}{SMT-LIB}: the satisfiability modulo theories library, used by SPF
  \item CORAL~\cite{CORAL-NFM11}: used by SPF
\end{itemize}
\fi
% ---------------------------------------------------------------------------------------------------

% ---------------------------------------------------------------------------------------------------
\iffalse
\subsection{Lazy evaluation}\mynote{citations?}
\mynote{[D] original text starts here} A technique used in practice to avoid frequent invocation of the constraint solver is {\em lazy evaluation}. The main idea is to avoid checking for contradictions at each branch condition, exploring both branches. This means that some of the states that the engine will later explore may be non-reachable during a real program execution. This allows a symbolic engine to quickly explore paths but forces it to check their consistency before using them for drawing some conclusions. The main disadvantages of this techniques are: (a) this strategy may lead to a large number of active states, (b) the time spent for checking the consistency of many states may be similar to the time spent for performing pruning in the first place.
\fi

\iffalse

\subsubsection{Constraint caching} Cache solver results and reuse them \mynote{to be completed}

\subsubsection{Constraint independence} tracks constraints into multiple independent subsets of constraints, This helps the system discards irrelevant constraints and adds additional cache hits.  \mynote{to be completed}

\subsubsection{Irrelevant constraint optimization} Remove from path constraints those constraints that are irrelevant in deciding the outcome of the current branch. In practice, this is done by computing the transitive closure of all the constraints. Pointer and array reference can make this hard: e.g., see details in~\cite{EXE-CCS06,EGL-ISSTA09,CUTE-FSE05}. In~\cite{KLEE-OSDI08}, this is called {\em constraint independence}: the main idea is to divide constraints in independent disjoint subsets based on the symbolic variables which they reference. Irrelevant constraints can detected and discarded.

\subsubsection{Incremental solving} Many paths common branches, then it can be beneficial to reason about subset or superset of constraints. See more details, e.g., in~\cite{KLEE-OSDI08,CUTE-FSE05}. In~\cite{KLEE-OSDI08}, they propose {\em counterexample caching} to keep a cache of counterexample based on past queries:
      \begin{itemize}
        \item if a subset of constraints has not solution, any superset does not have as well
        \item if a superset has a solution, any subset has a solution
        \item if a subset has a solution, try it for the superset
      \end{itemize}

\subsubsection{Bit-field theory expression} \cite{CKC-TOCS12} tries to simplify expression using a bit-field theory expressions: if parts of a symbolic variable are masked away by bit operations, then known bits of the symbolic variable can be replaced by their constant values. If all bits of a symbolic variable are constant, then the variable is marked as concrete.

\fi

%%%%%% Under-constrained symbolic execution

\begin{comment}
\paragraph{Under-constrained KLEE}\mynote{I will revise this (emilio)} In~\cite{UCKLEE-USEC15}, KLEE~\cite{KLEE-OSDI08} has been extended in order to support under-constrained symbolic execution . In particular, these are some of the main improvements:
\begin{itemize}
  \item {\em Lazy initialization.} Whenever there is a pointer that is not concrete and without any active constraint (i.e., it is unbound), its value is checked against {\tt NULL} then two path are analyzed: (a) where the pointer is {\tt NULL} and (b) where the pointer is pointing to a freshly allocated block of memory, whose content is marked as unbound.This means that pointer aliasing is assumed to not occur. In both paths, the pointer is not longer unbound.  In path (b), any test against the pointer is known and dereferences will be successfully resolved. Lazy initialization is common for data structure and thus it is common to bound its maximum length (i.e., {\em k-bounding}) in order to prevent the engine from allocating an unbounded number of objects.
  \item {\em Patch checking.} The main goal of~\cite{UCKLEE-USEC15} is to detect if a patch has introduced new bugs. In order to do so, it symbolically executes two compiled versions of a function: $P$, the unpatched version, and $P'$, the patched version. If it finds any execution paths along which $P'$ crashes but $P$ does not (when given the samce symbolic inputs), it reports a potential bug. Indeed, due to missing input preconditions, not all crashes are real bugs: if both $P$ and $P'$ crash on an input, then maybe the crash is given by the unknown preconditions. 
  \item {\em Pruning techniques.} Using a static cross-checker (that navigates the control-flow graph and marks differing basic block between $P$ and $P'$), \cite{UCKLEE-USEC15} prunes paths that have never executed a differing basic block and that cannot reach a differing basic block from their current program counter and call stack. Moreover, $P'$ is executed before executing $P$, allowing the system to prune paths that return from $P'$ without triggering an error, or that trigger an error without reaching different blocks.
  \item {Dealing with false positives.} Two approaches are pursued to limit false positives:
    \begin{itemize}
      \item {\em manual annotations}: examples are data types invariants or preconditions upon function calls.
      \item {\em automated heuristics}: {\em must-fail} heuristics identify errors that must occur for all input values following that execution path. For instance, {\em belief-fail} heuristic checks if a function contradicts itself (e.g., a code checks that a pointer is {\tt NULL} and then dereferences it). Another variation of must-fail heuristic is {\em concrete-fail} that an assertion failure or memory errors was triggered by a concrete condition or pointer.
    \end{itemize}
  \item {\em Rule-based checkers.} Several rule-based checkers have been built on top of UCKLEE. They do a similar job such as other dynamic tools (e.g., {\em memcheck} for memory leaks and uninitialized data) but reasoning on all possible paths, not just the concrete ones. Moreover, user inputs can be considered as fully constrained (i.e., no assumption is valid on it since the code should sanitize it). 
  \item {\em Optimizations.} Some optimizations:
    \begin{itemize}
      \item Symbolic objects have a symbolic size: whenever there is an access to the object content, the system verifies if the offset could exceed the object's symbolic size. whenever path is considered where the offset does not exceed the symbolic size, then a lower bound on the symbolic size is set.
      \item Some library functions (e.g., {\tt strlen}) have been replaced with variants that do not lead to path explosion
      \item Scores of rules to simplify symbolic expressions
      \item {\em lazy constraints}: defer evaluation of constraints using a solver as much as possible. For instance, if there is an hard constraint on branch, take both branches and if an error is found check if that branch was actually valid.
      \item function pointers should be made concrete by the user.
    \end{itemize}

\end{itemize}
\end{comment}


% ---------------------------------------------------------------------------------------------------
\subsection{Constraint optimizations}
\label{constraint-optimizations}

\mynote{add \cite{S-FMCAD08} + some optimizations listed in Section~\ref{memory-model}.}

Many optimization can be applied to constraints in order to make it more solver-friendly.

\mynote{[D] TOCS page 24}\subsubsection{Page boundings on pointers} Whenever a memory access with a symbolic pointer occurs,~\cite{CKC-TOCS12} determines the page referenced by the pointer. This information is then passed to the solver to help it. To make this even more effective, the page size is reduced as much as possible (e.g., 128 bytes).

%{\em Concolic execution} has been originally introduced in~\cite{DART-PLDI05} and then refined by~\cite{CUTE-FSE05}. A common disadvantage of symbolic execution is that the state space can be exponential. Moreover, even when the state space is tractable it may happen that complex constraints need to be solved but these constraints are too complex for the actual solver (e.g., non-linear constraints are typically hard to solve). For this reason, it is common to exploit concolic execution. 


% ---------------------------------------------------------------------------------------------------
\subsection{Once in Loops}

%\begin{comment}
Many prior works have targeted the problem of mitigating the path explosion effect due to symbolic execution of loops. We briefly discuss the main ideas introduced by these papers. 
\begin{itemize}

  \item {\em preconditions}: the symbolic execution of a loop can be made easier if some preconditions are known on the symbolic variables involved in the loop. For instance, if the number of loop iterations is known, the engine can drastically prune branching paths. For instance, this information may be determined using some static analysis techniques. Section~\ref{precontioned-symbolic-execution} provides a more general discussion of how preconditions can help symbolic execution.

  \item {\em fixed vs fully exploration}: depending on the goal, a symbolic engine may decide to fully explore a loop (e.g., see heuristics presented in~\cite{AEG-NDSS11} and discussed in Section~\ref{heuristics}) or to explore only a fixed number of iterations (e.g., up to 3 iterations) in order to avoid path explosion.

  \item {\em approximations}: effects of a loop are often approximated using {\em fixpoints} (e.g., in~\cite{KKM-USEC05,BNS-SP06,CFB-ACSAC06}). A fixpoint F is an approximation of the effect of loop body on an execution state. F approximates the state after the execution of loop whenever the initial state before the loop was F (?). Transforming an execution state to a fixpoint state is defined as widening. Construction of the fixpoint:
  \begin{itemize}
    \item S1: state after first iteration
    \item S2: state after second iteration
    \item compare S1 and S2: assign bottom to each symbol that has been altered
    \item repeat until there is no difference between Si and Si+1
    \item if there is a branch inside the loop, then either the branch is known or its condition is on a symbol which has been assign to bottom. In this case, two parallel states are created and then compared.
  \end{itemize}

  \item {\em loop invariant symbolic execution (LISE)}: Loop invariants can be discovered automatically using iterative techniques such as explained in~\cite{PV-SPIN04}, through the use of invariant straightening and approximation. The main idea is to work backward from a property that should be checked and then systematically applies approximation to reach termination. This approach has been later extended for parallel programs in~\cite{SZ-VMCAI12}.

  \item {\em loop summarization}: \cite{GL-ISSTA11} presents a technique that automatically derives partial summarizations for loop executions. A loop summarization is formalized similarly to a function summary (see Section~\ref{function-summaries}), using a set of preconditions $pre_{loop}$ and a set of postconditions $post_{loop}$. These are computed dynamically during the symbolic execution by reasoning on the dependency among loop conditions and symbolic variables. As soon as a loop summary is computed, it is cached for possibly subsequent reuse. This not only allows the symbolic engine to avoid redundant executions of the same loop under the same program state, but also make it possible to generalize the loop summary to cover even different executions of the same loop that run under different conditions. A main limitation of this approach is that it can generate summaries only for loops that interactively manipulate symbolic variable by a constant non-zero amount.

  \item {\em loop-extended symbolic execution (LESE)}: \cite{SPM-ISSTA09} has introduced a novel technique called LESE that symbolically tracks {\em trip counts} (i.e., number of times each loop is executed) and relate this information to features of the program input. A practical drawback of this technique is that a specification of the input grammar must be provided by the user to the symbolic execution engine.

  \item {\em compact symbolic execution}: \cite{SST-ATVA13} has introduced a technique that analyzes cyclic paths in the control flow graph of a given program and generates {\tt templates} that declarative describe the program states generated by these portions of code into a symbolic execution tree. By exploiting these templates, the symbolic execution engine needs to explore a significant reduced number of program states. A drawback of this approach is that templates introduce quantifiers into the path constraints. In turn, this can significantly increase the burden on the constraint solver.

  \item \cite{ST-ISSTA12} and~\cite{OT-ATVA11} present two technique for driving the symbolic execution of program toward a given target, even in presence of cyclic paths such as loops.\mynote{[E] non so se vale la pena discutere i dettagli}
\end{itemize}

Notice that detection/analysis of loops can be done using techniques such as~\cite{SGL-TOPLAS96} (e.g., in~\cite{CFB-ACSAC06}).
%\end{comment}

% ---------------------------------------------------------------------------------------------------
\subsection{Once in Path Explosion}

\begin{comment}
\begin{itemize}

  \item \cite{AEG-NDSS11}:
  \begin{itemize}
    \item {\em buggy-path-first}: priority to path that shown to contain errors (even if not exploitable)
    \item {\em loop exhaustion}: give priority to path that are exhausting a loop. In practice this can hit exploitable bugs (buffer overflows), but can prevent progress. Allow only one executor that is exhausting a loop, perform aggressive preconditioned symbolic execution.
  \end{itemize}

  \item {\em less covered code}: \cite{EXE-CCS06} uses a mixture of best-first and depth-first search. Best-first approach uses heuristic that give high priority to the path which is blocked at the line that has been executed the fewest number of times. The picked path is executed with DFS for a limited amount of time in order to avoid starvation. 

  \item \cite{KLEE-OSDI08} interleaves in a round robin fashion these strategies:
  \begin{itemize}
    \item {\em random path selection}: build a binary tree structure of all the state (each state is always created due to a fork from a parent). Assign same probability of being executed among states of the same subtree. Avoid starvation by given priority to states high in the tree.
    \item {\em coverage optimize search}: assign weights based on how much new code has been covered by a path. Pick up state randomly using weights as probability.
  \end{itemize}
  Each state is executed only for a time slice defined both as maximum number of instructions and as maximum amount of time.

  \item \cite{SAGE-NDSS08}:

  \item \cite{MAYHEM-SP12} same heuristics as~\cite{SAGE-NDSS08} and~\cite{KLEE-OSDI08}:
  \begin{itemize}
    \item executors exploring new code have high priority
    \item executors that identify symbolic memory accesses have high priority
    \item executors where symbolic instruction pointers are detected have high priority
  \end{itemize}

  \item \cite{CS-CACM13} mentions that a {\em fitness function} can be used to drive exploration of input space. Some examples: \cite{BHH-ASE11,LMH-JSS10}.

  \item Priority-based selection in~\cite{CKC-TOCS12}:
    \begin{itemize}
      \item bread-first search
      \item depth-first search
      \item max coverage heuristic
      \item path killer heuristic: kill paths that are no longer of interest (e.g., kill path that repeats sequence of program counters more than $n$ times)
    \end{itemize}

\end{itemize}
\end{comment}


%%% previous draft below %%%

\begin{comment}
Many prior works have targeted the problem of mitigating the path explosion effect due to symbolic execution of loops. We briefly discuss the main ideas introduced by these papers. 
\begin{itemize}

  \item {\em preconditions}: the symbolic execution of a loop can be made easier if some preconditions are known on the symbolic variables involved in the loop. For instance, if the number of loop iterations is known, the engine can drastically prune branching paths. For instance, this information may be determined using some static analysis techniques. Section~\ref{precontioned-symbolic-execution} provides a more general discussion of how preconditions can help symbolic execution.

  \item {\em fixed vs fully exploration}: depending on the goal, a symbolic engine may decide to fully explore a loop (e.g., see heuristics presented in~\cite{AEG-NDSS11} and discussed in Section~\ref{heuristics}) or to explore only a fixed number of iterations (e.g., up to 3 iterations) in order to avoid path explosion.

  \item {\em approximations}: effects of a loop are often approximated using {\em fixpoints} (e.g., in~\cite{KKM-USEC05,BNS-SP06,CFB-ACSAC06}). A fixpoint F is an approximation of the effect of loop body on an execution state. F approximates the state after the execution of loop whenever the initial state before the loop was F (?). Transforming an execution state to a fixpoint state is defined as widening. Construction of the fixpoint:
  \begin{itemize}
    \item S1: state after first iteration
    \item S2: state after second iteration
    \item compare S1 and S2: assign bottom to each symbol that has been altered
    \item repeat until there is no difference between Si and Si+1
    \item if there is a branch inside the loop, then either the branch is known or its condition is on a symbol which has been assign to bottom. In this case, two parallel states are created and then compared.
  \end{itemize}

  \item {\em loop invariant symbolic execution (LISE)}: Loop invariants can be discovered automatically using iterative techniques such as explained in~\cite{PV-SPIN04}, through the use of invariant straightening and approximation. The main idea is to work backward from a property that should be checked and then systematically applies approximation to reach termination. This approach has been later extended for parallel programs in~\cite{SZ-VMCAI12}.

  \item {\em loop summarization}: \cite{GL-ISSTA11} presents a technique that automatically derives partial summarizations for loop executions. A loop summarization is formalized similarly to a function summary (see Section~\ref{function-summaries}), using a set of preconditions $pre_{loop}$ and a set of postconditions $post_{loop}$. These are computed dynamically during the symbolic execution by reasoning on the dependency among loop conditions and symbolic variables. As soon as a loop summary is computed, it is cached for possibly subsequent reuse. This not only allows the symbolic engine to avoid redundant executions of the same loop under the same program state, but also make it possible to generalize the loop summary to cover even different executions of the same loop that run under different conditions. A main limitation of this approach is that it can generate summaries only for loops that interactively manipulate symbolic variable by a constant non-zero amount.

  \item {\em loop-extended symbolic execution (LESE)}: \cite{SPM-ISSTA09} has introduced a novel technique called LESE that symbolically tracks {\em trip counts} (i.e., number of times each loop is executed) and relate this information to features of the program input. A practical drawback of this technique is that a specification of the input grammar must be provided by the user to the symbolic execution engine.

  \item {\em compact symbolic execution}: \cite{SST-ATVA13} has introduced a technique that analyzes cyclic paths in the control flow graph of a given program and generates {\tt templates} that declarative describe the program states generated by these portions of code into a symbolic execution tree. By exploiting these templates, the symbolic execution engine needs to explore a significant reduced number of program states. A drawback of this approach is that templates introduce quantifiers into the path constraints. In turn, this can significantly increase the burden on the constraint solver.

  \item \cite{ST-ISSTA12} and~\cite{OT-ATVA11} present two technique for driving the symbolic execution of program toward a given target, even in presence of cyclic paths such as loops.\mynote{[E] non so se vale la pena discutere i dettagli}
\end{itemize}

Notice that detection/analysis of loops can be done using techniques such as~\cite{SGL-TOPLAS96} (e.g., in~\cite{CFB-ACSAC06}).
\end{comment}



%%%%% TAKEN FROM executors.tex

%
%Going from the concrete to the symbolic domain is easy: the called function is executed both concretely and symbolically, with the result from the former simply returned to the concrete domain, while symbolic execution explores the component of interest in full by turning the arguments for the call into purely symbolic ones.
%
%Going instead from the symbolic to the concrete domain, the arguments for a call are concretized and added to the path constraints, as the return value and any possible side effects are a consequence of the values chosen for the arguments. This may result in missing future paths because of the constraints over the arguments values or the return value and side effects. The engine marks such constraints as {\em soft}: whenever a branch in the symbolic domain is made inoperative by a soft constraint, the execution backtracks and a different choice of arguments is attempted, possibly leveraging branch conditions tracked during the concrete execution.
%
%\mynote{I: inglese non fluido. and side effects a che si riferisce? Si intende because of side effects? Girare frase (prima side effects poi constraints)?}  
%
%The concrete-symbolic boundary traversal is ruled by an {\em execution consistency model} that balances path over- and under-approximation (i.e., false negatives/positives) with scalability. {\sc \stwoe} offers six models targeting different use cases (e.g., systematic testing, reverse engineering, analysis of cryptographic code with hard-to-solve constraints). 
%
%A transition from the symbolic domain is more complicated:
%
%Executions within the symbolic and concrete domains are dubbed multi-path and single-path, respectively.


%%% v0
%\myparagraph{Selective Symbolic Execution} \mytempedit{{\sc \stwoe}~\cite{CKC-TOCS12} takes a different approach to issues (\ref{it:library-call-issue}), (\ref{it:third-party-issue}), and (\ref{it:smt-solver-issue}), by interleaving portions of code that are concretely run with fully symbolic phases.} The interleaving must be done carefully in order to preserve the meaningfulness of the whole exploration. When an argument $x$ for a function call to be concretized is symbolic, the engine converts it to some concrete value in order to perform the call, which is equivalent to corseting the exploration to a single path in the callee. When the call returns and the symbolic phase resumes, the concrete value for $x$ becomes part of the path constraints for the remainder of the exploration. However, a large number of paths may be then excluded. {\sc \stwoe} presents a systematic approach to consistently cross the symbolic/concrete boundary in both directions: it describes a strategy to deal with constraints introduced on symbolic values as a consequence of concretization, and introduces a number of consistency models -- where a state is {\em consistent} when there exists a feasible path to it from the initial state -- which suit different analyses. Constraints that are updated to account for concrete values are marked as {\em soft}. Whenever a branch is made inoperative by such a constraint in the symbolic territory, execution goes back and picks a value for the concrete call that would enable that branch. %\mytempedit{Similarly to dynamic symbolic execution, this technique may incur in false negatives due to concretization.}

%Other techniques that could help reduce false negative will be later discussed in Section~\ref{se:constraint-solving}.

%\myparagraph{Execution-Generated Testing}\mynote{Is \cite{DBF-ISSTA16} crucial?} \mytempedit{A common concolic approach used for mitigating issues (\ref{it:smt-solver-issue}) and (\ref{it:symbolic-performance-issue}) lets the symbolic engine always execute concretely the operations that involve only concrete values. This idea, adopted, e.g., by {\sc KLEE}~\cite{KLEE-OSDI08}, {\sc EXE}~\cite{EXE-CCS06}, and~\cite{CS-CACM13}, can substantially speed up the computation and make it possible to reason even over complex (e.g., non-linear) operations if they involve only concrete values. We defer to Section~\ref{se:constraint-solving} an additional discussion of how concolic execution can help to cope with issue~(\ref{it:smt-solver-issue}). }

%Throughout this article, we will see other uses of concretization (see, e.g., Section~\ref{memory-model} and Section~\ref{se:constraint-solving})  and of concolic execution (see Section~\ref{se:path-explosion}).

% add something...


%\myparagraph{Dynamic Test Generation} 


%\mynote{Add paper on hybrid concolic testing}In particular,

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Old sections about concolid execution and dynamic symbolic execution
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Concrete, Symbolic, and Concolic Execution}
% \label{ss:concrete-concolic-symbolic}

% As shown in the warm-up example (Section~\ref{symbolic-execution-example}), a symbolic execution of a program can generate -- in theory -- all possible control flow paths that the program could take during its concrete executions on specific inputs. While modeling all possible runs allows for very interesting analyses, it is typically unfeasible in practice, especially on real-world software, for a variety of reasons.

% \mytempedit{First, as extensively discussed in Section~\ref{se:path-explosion}, the number of control flow paths to be generated and analyzed could be prohibitively large, due to branch instructions and loops. In the worst case, if the code contains an unbounded loop, symbolic execution could keep running forever, generating a potentially infinite number of paths (we refer to Section~\ref{se:path-explosion} for an example).}

% Moreover, as observed in Section~\ref{se:intro}, symbolic engines are clients of SMT solvers, which are continuously invoked during the analysis. Although powerful SMT solvers are currently available, the time spent in constraint solving is still one of the main performance barriers for symbolic engines. It may also happen that the program yields constraints that the solver cannot handle well (e.g., non-linear constraints), in spite of the fact that symbolic executors often use more than one solver in order to support as many decidable logical fragments as possible.
% %An SMT instance is a formula in first-order logic, where some function and predicate symbols have additional meaning. This meaning depends on the theory being used: for instance, with linear inequalities symbols with extra meaning include the integers, $+$, $-$, $x$, $\le$. Path constraints are typically expressed in a decidable logical fragment without quantifiers

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.35\columnwidth]{images/concrete-abstract.eps} 
% \caption{Concrete and abstract execution machine models.}
% \label{fig:concrete-symbolic}
% \end{figure}

% A standard approach to limit the resources (running time and space usage) required by the execution engine and to handle complex constraints is to mix concrete and symbolic execution: this is dubbed {\em concolic execution}, where the term concolic is a portmanteau of the words concrete and symbolic. The basic idea is to have the concrete execution drive the symbolic execution (see also Figure~\ref{fig:concrete-symbolic}). Besides the symbolic store and the path constraints, a concolic execution engine also maintains a concrete store $\sigma_c$. After choosing an arbitrary input to begin with, it executes the program both concretely and symbolically by simultaneously updating the two stores and the path constraints. In order to explore different paths, the path conditions given by one or more branches can be negated and the SMT solver invoked to find a satisfying assignment for the new constraints, i.e., to generate a new input.

% %\mynote{[D] path conditions starting from the last one?}

% %\begin{figure}[t]
% %  \centering
% %  \includegraphics[width=1.0\columnwidth]{images/execution-tree.eps} 
% %  \caption{TREE TO BE REPLACED. Concolic execution of function {\tt foobar} given in Figure~\ref{fig:example-1} starting with input $a = 1$ and $b = 1$. Besides the statement to be executed, the symbolic store $\sigma$, the path constraints $\pi$, execution states also show the concrete store $\sigma_c$. }
% %%For the sake of presentation the conjunction of constraints is shown as a list of constraints. }
% %  \label{fig:example-concolic-execution}
% %\end{figure}

% \vspace{-2pt} % TODO
% \boxedexample{
% Consider the C function in Figure~\ref{fig:example-1} and suppose to choose $a = 1$ and $b = 1$ as input parameters. Under these conditions, the concrete execution takes path $A\leadsto B\leadsto C\leadsto E\leadsto G$ in the symbolic tree of Figure~\ref{fig:example-symbolic-execution}. Besides the symbolic stores shown in Figure~\ref{fig:example-symbolic-execution}, the concrete stores maintained in the traversed states are the following:
%   \begin{enumerate}
  
%   \item[]$-$~~$\sigma_c=\{a\mapsto 1,~b\mapsto 1\}$ in state $A$;
%   \item[]$-$~~$\sigma_c=\{a\mapsto 1,~b\mapsto 1,~x\mapsto 1,~y\mapsto 0\}$ in states $B$ and $C$;
%   \item[]$-$~~$\sigma_c=\{a\mapsto 1,~b\mapsto 1,~x\mapsto 1,~y\mapsto 4\}$ in states $E$ and $G$.
  
%   \end{enumerate}  
% %Stores and path constraints maintained by the concolic run are shown in Figure~\ref{fig:example-concolic-execution}. 
% After checking that the \texttt{assert} conditions at line 8 succeed, we can generate a new control flow path by negating the last path constraint, i.e., $\alpha_b\neq 0$. The solver at this point would generate a new input that satisfies the constraints $\alpha_a\neq 0\,\wedge\, \alpha_b=0$ (for instance $a = 1$ and $b = 0$) and the execution would continue in a similar way along the path $A\leadsto B\leadsto C\leadsto E\leadsto F$. %
% }

% %\vspace{2mm}
% %\noindent
% %\fbox{%
% %  \parbox{0.98\textwidth}{%
% %  {\em Example.}  Consider the C function in Figure~\ref{fig:example-1} and suppose to choose $a = 1$ and $b = 1$ as input parameters. Under these conditions, the concrete execution takes path $A\leadsto B\leadsto C\leadsto E\leadsto G$ in the symbolic tree of Figure~\ref{fig:example-symbolic-execution}. Besides the symbolic stores shown in Figure~\ref{fig:example-symbolic-execution}, the concrete stores maintained in the traversed states are the following:
% %  \begin{enumerate}
% %  
% %  \item[]$-$~~$\sigma_c=\{a\mapsto 1,~b\mapsto 1\}$ in state $A$;
% %  \item[]$-$~~$\sigma_c=\{a\mapsto 1,~b\mapsto 1,~x\mapsto 1,~y\mapsto 0\}$ in states $B$ and $C$;
% %  \item[]$-$~~$\sigma_c=\{a\mapsto 1,~b\mapsto 1,~x\mapsto 1,~y\mapsto 4\}$ in states $E$ and $G$.
% %  
% %  \end{enumerate}  
% %%Stores and path constraints maintained by the concolic run are shown in Figure~\ref{fig:example-concolic-execution}. 
% %After checking that the \texttt{assert} conditions at line 8 succeed, we can generate a new control flow path by negating the last path constraint, i.e., $\alpha_b\neq 0$. The solver at this point would generate a new input that satisfies the constraints $\alpha_a\neq 0\,\wedge\, \alpha_b=0$ (for instance $a = 1$ and $b = 0$) and the execution would continue in a similar way along the path $A\leadsto B\leadsto C\leadsto E\leadsto F$. }%
% %}

% %\vspace{2mm} % TODO rimosso
% \vspace{-2pt}  % TODO was +2mm
% \noindent As shown by the example, the symbolic information maintained during a concrete run can be exploited by the execution engine, for instance, to obtain new inputs and explore new control flow paths. We will further discuss this aspect in Section~\ref{ss:heuristics}. 

% It is worth noticing that concolic execution trades soundness for performance: false negatives are indeed possible, because some program executions -- and therefore possible erroneous behaviors -- may be missed. In the literature, this is also regarded as an {\em under-approximate} form of program analysis.

% % with fully symbolic phases, enabling program analysis across a full software stack without sacrificing scalability
% Many papers exploit variants of concolic execution or different ways of mixing concrete and symbolic runs. \mytempedit{\cite{DBF-ISSTA16} presents a framework for the specification of mixed concrete/symbolic policies, allowing for a better understanding of their impact and trade-offs in terms of correctness, completeness and efficiency of the approach.} For instance, in {\em execution-generated testing} (see, e.g., {\sc KLEE}~\cite{KLEE-OSDI08}, {\sc EXE}~\cite{EXE-CCS06}, and~\cite{CS-CACM13}, the symbolic engine always executes concretely the operations that involve only concrete values. This makes it possible to reason even over complex (e.g., non-linear) operations if they involve only concrete values. {\em Selective symbolic execution}~\cite{CKC-TOCS12} takes a different approach, by interleaving portions of code that are concretely run with fully symbolic phases. The interleaving must be done carefully in order to preserve the meaningfulness of the whole exploration. When an argument $x$ for a function call to concretize is symbolic, the engine should convert it to some concrete value in order to perform the call, which is equivalent to corseting the exploration to a single path in the callee. When the call returns and the symbolic phase resumes, the concrete value for $x$ becomes part of the path constraints for the remainder of the exploration. However, a large number of paths may be then excluded. {\sc \stwoe}~\cite{CKC-TOCS12} presents a systematic approach to consistently cross the symbolic/concrete boundary in both directions: it describes a strategy to deal with constraints introduced on symbolic values as a consequence of concretization, and introduces a number of consistency models -- where a state is {\em consistent} when there exists a feasible path to it from the initial state -- which suit different analyses. Constraints that are updated to account for concrete values are marked as {\em soft}. Whenever a branch is made inoperative by such a constraint in the symbolic territory, execution goes back and picks a value for the concrete call that would enable that branch. Throughout this article, we will see other uses of concretization (see, e.g., Section~\ref{memory-model} and Section~\ref{se:constraint-solving})  and of concolic execution (see Section~\ref{se:path-explosion}).

% %===================================================================================
% \subsection{Dynamic Symbolic Execution} 
% \label{ss:dynamic-symbolic-execution}

% %\myparagraph{Dynamic Test Generation} 
% Traditional symbolic execution does not scale over large programs. Although search heuristics may help prioritize some interesting paths, symbolic execution may still proceed extremely slow. Indeed, the engine must simulate any instruction in the program and heavily relies on the constraint solver in order to make any progress in the execution. {\em Dynamic test generation}, initially introduced in {\sc DART}~\cite{DART-PLDI05}, is a technique that can help symbolic execution scale to large programs. The main idea is to execute a program both concretely and symbolically. This kind of execution is often referred to as concolic execution (Section~\ref{ss:concrete-concolic-symbolic}). Initially, a random input is generated and a concrete execution is started. In parallel, a symbolic execution is also started. Whenever the concrete execution takes a branch, the symbolic execution is directed toward the same branch and the constraints extracted from the branch condition are added to the current set of path constraints. In other words, the symbolic execution is driven by a specific concrete execution. A consequence of this approach is that the symbolic engine does not need to invoke the constraint solver to decide whether a branch condition is (un)satisfiable, since this is directly tested by the concrete execution. Other paths of the program execution can be then explored by selecting a previously taken branch and negating its constraints. Using a constraint solver, the symbolic engine can generate a new (random) input that drives the concrete execution toward a new path. This strategy can be repeated as much as needed to achieve the desired coverage.

% \begin{figure}[t]
%   %\vspace{-3mm}
%   %\centering
%   \begin{subfigure}{.29\textwidth}
%     \vspace{5mm}
%     \begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
% int bar(int x, int y) {
%    if (x < 5 && y >= 6) 
%       if (x > 1) return 1;
%       else return 2;
%    else return 3;
% }
%     \end{lstlisting}
%     \vspace{5mm}
%     \caption{}
%   \end{subfigure}%
%   \hspace{-2mm}
%   \begin{subfigure}{.37\textwidth}
%     \centering
%     \includegraphics[width=1.0\columnwidth]{images/concolic-execution} 
%     %\label{fig:sub1}
%     \caption{}
%   \end{subfigure}%
%   \hspace{-2mm}
%   \begin{subfigure}{.33\textwidth}
%     \centering
%     \vspace{0.03mm}
%     \includegraphics[width=1.1\columnwidth]{images/concolic-execution-2} 
%     %\label{fig:sub2}
%     %\vspace{0.0mm}
%     \caption{}
%   \end{subfigure}
%   \vspace{-2mm}
%   \caption{Dynamic test generation example. (a) Source code of function {\tt bar}. (b) Symbolic execution tree for the function {\tt bar}. Solid edges show the path taken by the concolic execution when {\tt x = 2} and {\tt y = 6}. These input values have been randomly chosen. (c) Concolic execution when {\tt x = 1} and {\tt y = 6}. These input values have been obtained using a constraint solver, after negating the path constraints of \iffullver{node B in the function {\tt bar}}{node B}.}
%   \label{fig:example-concrete-execution}
%   \vspace{-3mm}
% \end{figure}

% % pick the values {\tt x = 2} and {\tt y = 6}, respectively
% \vspace{-3pt} % TODO
% \boxedexample{ An example of dynamic test generation is shown in Figure~\ref{fig:example-concrete-execution}. Consider the function {\tt bar} (Figure~\ref{fig:example-concrete-execution}a) that takes two integer inputs {\tt x} and {\tt y}. To start a first concrete execution, a symbolic engine may initially randomly pick {\tt x = 2} and {\tt y = 6} as input values. The concrete execution induced by these inputs is presented in Figure~\ref{fig:example-concrete-execution}b: both the first and second branch condition (nodes $A$ and $B$) are satisfied, directing the execution toward the first {\tt return} statement (node $D$). Nodes $C$ and $E$ are skipped since their associated branch conditions are not met by the current input values. For instance, node $E$ is not executed since the condition $x > 1$ (node $B$) directs the path toward the node $D$. An engine that desires to symbolically execute a path containing the node $E$ must track the constraints during the concrete execution over {\tt x = 2} and {\tt y = 6} and then negate the branch condition $x > 1$. To generate a new input, the engine then invokes a solver over the constraints $\neg(\alpha_x > 1) \wedge (\alpha_x < 5 \wedge \alpha_y > 5)$, getting, e.g., {\tt x = 1} and {\tt y = 6} (Figure~\ref{fig:example-concrete-execution}c). Notice that since {\tt y} is not involved in the branch condition that is currently negated, the engine may reuse its value and include an additional constraint $\alpha_y = 6$. This optimization may drastically reduce the solving time required to obtain a solution from the constraint solver.}
% \vspace{-3pt} % TODO

% % that still need to be visited
% % what is done with
% \noindent Although dynamic test generation uses concrete inputs to drive the symbolic execution toward a specific path, it still needs to pick a branch to negate whenever a new path has to be explored. Notice also that each concrete execution may add new branches that will have to be visited. Since the set of non-taken branches across all the performed concrete executions can be very large, the search heuristics discussed in Section~\ref{ss:heuristics} still play a crucial role. For instance, {\sc DART}~\cite{DART-PLDI05} chooses the next branch to negate using a DFS strategy. Additional strategies for picking the next branch to negate have been presented in literature. For instance, the {\em generational search} algorithm discussed in {\sc SAGE}~\cite{SAGE-NDSS08} systematically yet partially explores the state space, maximizing the number of new tests generated while also avoiding redundancies in the search. This is achieved by negating constraints following a specific order and by limiting the backtracking of the search algorithm. Since the state space is only partially explored, the initial input plays a crucial role in the effectiveness of the overall approach.  The importance of the first input is similar to what happens in traditional {\em black-box fuzzing} and, for this reason, symbolic engines such as {\sc SAGE} are often referred as {\em white-box fuzzers}.

%\vspace{-1mm}
